---
title: "Lab 5"
author: "Yealim Sung and Elcy Timothy"
date: "10/31/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> Usernames: sunghailame, etimothy 

> Repo name: ds202_lab5

## Processing the data

1. Read the data into R (you can use the function read.table('diabetes.txt',header=TRUE). Provide a snapshot of the data.

2. Replace the missing values in the variable frame, indicated by an empty string ‘’, by ‘NA’. Also, use function droplevels() to remove empty categorical value ‘’ from frame.

3. Drop id, bp.2s, and bp.2d from our dataset. Call this dataset diabetes_reduced.

4. Using diabetes_reduced, drop any rows having any missing values. Call this new dataset diabetes_clean. For the remaining questions, use this diabetes_clean dataset. Your clean dataset should have 366 rows and 16 columns. To identify the row indices that have a missing value you can use this code:

index.na=apply(is.na(diabetes_reduced), 1, any) ## identify rows with missing value.
You’ll need to write additional code to remove these rows.

5. How can we check Step 4 was done correctly?

```{r}
library(dplyr)
library(ggplot2)
#1
df = read.table('diabetes.txt', header = TRUE)
#2
df[which(df$frame == ''), 'frame'] = NA
df$frame <- droplevels(df$frame)
#3
diabetes_reduced = select(df, -c("id", "bp.2s", "bp.2d"))
#4
diabetes_reduced[-c(index.na=apply(is.na(diabetes_reduced), 1, any))] ## identify rows with missing value.
diabetes_clean <- na.omit(diabetes_reduced)
#5
which(is.na(diabetes_clean))
```

## Exploring and transforming data

6. We see that glyhb is highly right skewed. How can we remedy this? What are some potential downsides to our approach? (throwback to Lecture 7 and fbi dataset). 

- We can fix the skewness by doing a log transformation. Though it will make the skew not as harh but it doesnt completely go away.

7. Based on your answer to (6), create a new variable called glyhb_star that is no longer right-skewed. Plot a histogram of glyhb_star to verify that the variable is symmetric. From now on use this glyhb_star variable.
```{r}
#7
diabetes_clean$glyhb_star = log(diabetes_clean$glyhb)
ggplot(diabetes_clean, aes(x=glyhb_star)) + geom_histogram() + labs(title="'glyhb_star' Variable", x="log(glyhb)")

```

8. Let’s explore some summary statistics before jumping into visualizations. As mentioned before, our main variable of interest is glyhb_star, which is transformed version of glyhb. Create some tables of summary statistics of glyhb_star grouped by variables you are interested in and may help illuminate which variables are associated with diabetes. Describe your findings. For example:

```{r}
diabetes_clean %>% group_by(frame) %>% summarise(mean.glyhb = mean(glyhb_star))
```

Note: Your glyhb_star might have different values than mine and that’s ok.

```{r}
#8
library(dplyr)
diabetes_clean %>% group_by(weight) %>% summarise(mean.glyhb = mean(glyhb_star))
diabetes_clean %>% group_by(height) %>% summarise(mean.glyhb = mean(glyhb_star))
diabetes_clean %>% group_by(gender) %>% summarise(mean.glyhb = mean(glyhb_star))

```


- Weight by mean.glybh does not really have as strong of a relationship as I thought it would. Initially I thought that as weight increased mean.glybh would increase; but there are higher weights that have lower mean.glybh than lower weights. As for height by mean.glyhb it's the same, doesn't really have a strong increase as far as mean.glybh increasing as height increases. And lastly, males have a slightely higher mean.glyhb than females. 

## Visualizations

10. The following table is difficult to unpack. Create a plot to visualize the information. (Hint: look at lec12_script_updated.R and recall what we did for the starwars dataset.)

```{r}
diabetes_clean %>% group_by(frame,location) %>% summarise (mean.glyhb_star= mean(glyhb_star))
```
```{r}
#10
new = diabetes_clean %>% group_by(frame,location) %>% summarise (mean.glyhb_star= mean(glyhb_star))
ggplot(new,aes(frame, mean.glyhb_star, group=location, color=location)) + geom_line() + labs(title="Mean log(glyhb) by frame")
```

11. Our main variable of interest is glyhb_star. We want to understand its relationship with ratio, bp.1s, age, gender, hip and weight. Further explore how these variables interact and visualize your findings.

```{r}
#11
ggplot(diabetes_clean,aes(x=ratio, y=glyhb_star)) + geom_point() + labs(title="Ratio by log(glyhb)")
ggplot(diabetes_clean,aes(x=bp.1s, y=glyhb_star)) + geom_point() + labs(title="Blood Pressure by log(glyhb)")
ggplot(diabetes_clean,aes(x=age, y=glyhb_star)) + geom_point() + labs(title="Age by log(glyhb)")
ggplot(diabetes_clean,aes(x=gender, y=glyhb_star)) + geom_boxplot() + labs(title="Gender by log(glyhb")
ggplot(diabetes_clean,aes(x=hip, y=glyhb_star)) + geom_point() + labs(title="Hip Measurement (in) by log(glyhb)")
ggplot(diabetes_clean,aes(x=weight, y=glyhb_star)) + geom_point() + labs(title="Weight by log(glyhb")
```


12. Write code to improve this plot so that we can see the distinct patterns for hip and waist across different frame and avoid overplotting. Propose two different plots.

```{r}
ggplot(diabetes_clean,aes(y=hip,x=waist,alpha=0.5)) + geom_point() + facet_wrap(~frame) 
```

```{r}
#12
ggplot(diabetes_clean, aes(y=hip, x=waist, color=frame, shape=frame, alpha=0.5)) + geom_point() + facet_wrap(~frame) + labs(title="Hip vs weight by Frame")
ggplot(diabetes_clean, aes(y=hip, x=waist, group=1)) + geom_boxplot() + facet_wrap(~frame) + labs(title="Hip vs weight by Frame")

```

## Messy data

13. gather and spread may seem slightly unnatural to use at first, but they are very powerful functions that can transform data into the right format. Explain in your own words what the gather and spread functions do.

- Gather would rearrange data by moving columns into rows and spread would move rows into columns.

14. Are gather and spread exact complements of each other? Explain.

- Since gather and spread does what the other function can't do, I would say it is the exact complements of each other. 

## Regression models

15. We fit the following linear regression model:

```{r}
fit = lm(glyhb_star ~stab.glu + age + waist + ratio+ factor(frame),data=diabetes_clean)
 summary(fit)
```

What insights can be obtain from this model? Explain clearly and make note of the F-statistic and adjusted R-squared. Do the results from our exploratory analysis suggest a linear model is the right approach here? You may reference plots and summary statistics from previous steps.

- We would say the relationship between the predictor and the response variables are good since the F-statistic value is 77.49 and it means the larger the number the better the relationship is. Since the adjusted R-squared value is on 0.557, that represents that roughly 56% of the variance in the response variables can be explained by the predictor variable. Therefore, we think the model is in the right approach here although the adjusted R-squared value can be slightly improved.

16. Interpret the estimated regression coefficient for each predictor in fit, regardless of whether or not the predictor is significant.

- Since the estimate slope values for stab.glu, age, waist, ratio, medium factor and small factor are close to value 0, it tells us that when the values are increased, the value for glyhb_star will increase as well but only slightly because of the low slope. Since the intercept value is 0.833 it tells us that glyhb_star will satisfy other values at 0.833.

17. We can see the estimated fitted values (Y^) from our model using the following code fit$fitted.values. These estimated fitted values are estimates of what true value? Based on the fit model, calculate the value of Y^ when stab.glu = 90, age = 35, waist = 30, ratio = 5.1, and frame = small.

```{r}
diabetes_fit = data.frame(stab.glu=90, age=35, waist=30, ratio=5.1, frame="small")
predict.lm(fit, diabetes_fit, interval = "prediction")
```

18. Explain the difference between inference and prediction.

- Inference is when the act or process of approaching to a conclusion is by using facts or evidence. It has to be made about facts using statistics, calculations, obervations or generalizations. Prediction is a statement about what will or might happen in the future. Therefore, the difference would be that inference is based on facts and evidence while prediction is based on some sort of fact, obervation, or experience and it may not be actually happening.

19. What are advantages/disadvantages to constructing a linear regression model as opposed to a k-NN regression model?

- Since KNN has to keep track of all training data and find the neighbor nodes, it is slow in real time. Opposed to that, Linear Regression model can easily extract output from the tuned θ coefficients which means it will have fast training time and spcae complex solution. However, linear regression can only support linear solutions and that gives us limitation. Also, linear regression model algorithm assumes that the input features are mutually independent.

## Reflection

20. On your very first HW, I asked you what you think data science is. Have your views changed?
Discuss with your partner what you have found to be (1) most surprising about data science, (2) most challenging, and (3) most enjoyable. Write a brief paragraph addressing these points.

-  I thought that the most surprising thing about data science for me (Elcy) was how much statistics is really involved.I think that a lot of things were difficult for me becuase I'm new to programming and I feel like a have a huge learning curve. Most enjoyable for me is when I understand how to use a function. 
On my first hw (Yealim), I said that data science is to study the data. Now that I have done several labs and hws, data science is not just simply studying the data. We have to understand and truly learn the relations between different data. I agree with Elcy on the most suprising thing about data science. The most challening part for me was to learn the relation and interpret the data from that. It was not easy when I just tried to simply work on the course works without taking my time to understand the dataset. The most enjoyable part was when I was able to plot pretty graphs. It was really satisfying to be able to do that.